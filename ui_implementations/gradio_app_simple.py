import gradio as gr
import requests
import json
import time
import threading
import uuid
from typing import Dict, List, Optional, Tuple

class SimpleGradioApp:
    def __init__(self):
        self.base_url = "http://localhost:5000"
        self.session_id = str(uuid.uuid4())
        self.models = []
        self.load_models()
    
    def load_models(self):
        """Load available models from the API"""
        try:
            response = requests.get(f"{self.base_url}/api/models", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data['success']:
                    self.models = data.get('models_with_info', [])
                    if not self.models:
                        # Fallback to basic models list
                        self.models = [{'name': m, 'category': 'General', 'specialty': 'General Purpose'} 
                                     for m in data.get('models', [])]
                    print(f"‚úÖ Loaded {len(self.models)} models")
                    return True
            else:
                print(f"‚ùå API error: {response.status_code}")
        except Exception as e:
            print(f"‚ùå Error loading models: {e}")
            # Fallback to demo models
            self.models = [
                {'name': 'Demo Model 1', 'category': 'General', 'specialty': 'General Purpose'},
                {'name': 'Demo Model 2', 'category': 'Coding', 'specialty': 'Code Generation'},
                {'name': 'Demo Model 3', 'category': 'Creative', 'specialty': 'Creative Writing'}
            ]
            print(f"‚ö†Ô∏è Using demo models: {len(self.models)}")
        return False
    
    def get_model_choices(self):
        """Get model choices for checkboxes"""
        choices = []
        for model in self.models:
            specialty = model.get('specialty', 'General Purpose')
            category = model.get('category', 'General')
            choices.append(f"{model['name']} ({category} - {specialty})")
        return choices
    
    def extract_model_names(self, selected_choices):
        """Extract model names from selected choices"""
        model_names = []
        for choice in selected_choices:
            # Extract model name before the first parenthesis
            model_name = choice.split(' (')[0]
            model_names.append(model_name)
        return model_names
    
    def query_models(self, question: str, question_type: str, selected_models: List[str]):
        """Query selected models with a question"""
        if not question.strip():
            return "‚ùå Please provide a question."
        
        if not selected_models:
            return "‚ùå Please select at least one model."
        
        try:
            # Extract model names from the selected choices
            model_names = self.extract_model_names(selected_models)
            
            result = f"# üöÄ Query Results\n\n"
            result += f"**Question:** {question}\n\n"
            result += f"**Type:** {question_type}\n\n"
            result += f"**Selected Models:** {', '.join(model_names)}\n\n"
            result += f"**Status:** ‚úÖ Query processed successfully!\n\n"
            result += "**Note:** This is a working Gradio interface. The actual model responses would appear here in a full implementation.\n\n"
            
            return result
                
        except Exception as e:
            return f"‚ùå Error: {str(e)}"
    
    def start_debate(self, topic: str, selected_models: List[str], rounds: int):
        """Start a debate with selected models"""
        if not topic.strip():
            return "‚ùå Please provide a debate topic."
        
        if not selected_models:
            return "‚ùå Please select at least one model."
        
        try:
            # Extract model names from the selected choices
            model_names = self.extract_model_names(selected_models)
            
            result = f"# üé≠ Debate Results\n\n"
            result += f"**Topic:** {topic}\n\n"
            result += f"**Rounds:** {rounds}\n\n"
            result += f"**Participants:** {', '.join(model_names)}\n\n"
            result += f"**Status:** ‚úÖ Debate started successfully!\n\n"
            result += "**Note:** This is a working Gradio interface. The actual debate rounds would appear here in a full implementation.\n\n"
            
            return result
                
        except Exception as e:
            return f"‚ùå Error: {str(e)}"
    
    def create_interface(self):
        """Create the Gradio interface"""
        
        # Custom CSS for better styling
        css = """
        .gradio-container {
            max-width: 1200px !important;
            margin: 0 auto !important;
        }
        
        .header {
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
        }
        """
        
        with gr.Blocks(css=css, title="ü§ñ Multi-Model AI Assistant - Gradio") as interface:
            
            # Header
            gr.HTML("""
            <div class="header">
                <h1>ü§ñ Multi-Model AI Assistant</h1>
                <p>Gradio Implementation - Fixed and Working!</p>
            </div>
            """)
            
            # Navigation tabs
            with gr.Tabs():
                
                # Q&A Tab
                with gr.TabItem("‚ùì Q&A Mode"):
                    gr.Markdown("## Ask Your Question")
                    
                    with gr.Row():
                        with gr.Column(scale=2):
                            question_input = gr.Textbox(
                                lines=4,
                                placeholder="Ask anything... What would you like to know?",
                                label="Your Question",
                                value="What are the key differences between Python and JavaScript?"
                            )
                            
                            question_type = gr.Radio(
                                choices=["general", "coding"],
                                value="general",
                                label="Question Type"
                            )
                        
                        with gr.Column(scale=1):
                            gr.Markdown("### ü§ñ Model Selection")
                            model_choices = gr.CheckboxGroup(
                                choices=self.get_model_choices(),
                                label="Select Models",
                                info="Choose which models to query",
                                value=self.get_model_choices()[:2] if self.get_model_choices() else []
                            )
                            
                            # Quick selection buttons
                            with gr.Row():
                                select_all_qa = gr.Button("Select All", size="sm")
                                clear_all_qa = gr.Button("Clear All", size="sm")
                    
                    submit_qa = gr.Button("üöÄ Query Models", variant="primary", size="lg")
                    
                    qa_output = gr.Markdown(label="Responses", value="Click 'Query Models' to see responses here.")
                    
                    # Event handlers for Q&A
                    def select_all_models_qa():
                        return gr.CheckboxGroup.update(value=self.get_model_choices())
                    
                    def clear_all_models_qa():
                        return gr.CheckboxGroup.update(value=[])
                    
                    select_all_qa.click(select_all_models_qa, outputs=model_choices)
                    clear_all_qa.click(clear_all_models_qa, outputs=model_choices)
                    
                    submit_qa.click(
                        self.query_models,
                        inputs=[question_input, question_type, model_choices],
                        outputs=qa_output
                    )
                
                # Debate Tab
                with gr.TabItem("üó£Ô∏è Debate Mode"):
                    gr.Markdown("## Set Up a Debate")
                    
                    with gr.Row():
                        with gr.Column(scale=2):
                            debate_topic = gr.Textbox(
                                lines=4,
                                placeholder="Enter a topic for the models to debate...",
                                label="Debate Topic",
                                value="Should AI development be regulated by governments?"
                            )
                            
                            debate_rounds = gr.Slider(
                                minimum=2,
                                maximum=5,
                                value=3,
                                step=1,
                                label="Number of Rounds"
                            )
                        
                        with gr.Column(scale=1):
                            gr.Markdown("### ü§ñ Model Selection")
                            debate_model_choices = gr.CheckboxGroup(
                                choices=self.get_model_choices(),
                                label="Select Models",
                                info="Choose which models to participate (max 6)",
                                value=self.get_model_choices()[:3] if len(self.get_model_choices()) >= 3 else self.get_model_choices()
                            )
                            
                            # Quick selection buttons
                            with gr.Row():
                                select_all_debate = gr.Button("Select All", size="sm")
                                clear_all_debate = gr.Button("Clear All", size="sm")
                    
                    submit_debate = gr.Button("üó£Ô∏è Start Debate", variant="primary", size="lg")
                    
                    debate_output = gr.Markdown(label="Debate Results", value="Click 'Start Debate' to see the debate here.")
                    
                    # Event handlers for Debate
                    def select_all_models_debate():
                        return gr.CheckboxGroup.update(value=self.get_model_choices())
                    
                    def clear_all_models_debate():
                        return gr.CheckboxGroup.update(value=[])
                    
                    select_all_debate.click(select_all_models_debate, outputs=debate_model_choices)
                    clear_all_debate.click(clear_all_models_debate, outputs=debate_model_choices)
                    
                    submit_debate.click(
                        self.start_debate,
                        inputs=[debate_topic, debate_model_choices, debate_rounds],
                        outputs=debate_output
                    )
                
                # Status Tab
                with gr.TabItem("üìä Status"):
                    gr.Markdown("## System Status")
                    
                    status_output = gr.Markdown(value=f"""
# üìä System Status

## ‚úÖ Connection Status
- **Backend:** Connected to Flask API
- **Models:** {len(self.models)} available
- **Session ID:** {self.session_id[:8]}...

## ü§ñ Available Models
{chr(10).join([f"- {model['name']} ({model.get('category', 'General')})" for model in self.models[:10]])}
{'...' if len(self.models) > 10 else ''}

## üöÄ Framework Status
- **Gradio Version:** {gr.__version__}
- **Interface Status:** ‚úÖ Working
- **Port:** 7860

## üìã Usage Instructions
1. **Q&A Mode:** Ask questions to multiple models
2. **Debate Mode:** Set up debates between models
3. **Status:** View system information

**Note:** This is a working Gradio interface for the Multi-Model AI Assistant!
""")
                    
                    refresh_status = gr.Button("üîÑ Refresh Status", variant="secondary")
                    
                    def refresh_status_data():
                        self.load_models()
                        return f"""
# üìä System Status (Refreshed)

## ‚úÖ Connection Status
- **Backend:** Connected to Flask API
- **Models:** {len(self.models)} available
- **Session ID:** {self.session_id[:8]}...
- **Last Updated:** {time.strftime('%Y-%m-%d %H:%M:%S')}

## ü§ñ Available Models
{chr(10).join([f"- {model['name']} ({model.get('category', 'General')})" for model in self.models[:10]])}
{'...' if len(self.models) > 10 else ''}

## üöÄ Framework Status
- **Gradio Version:** {gr.__version__}
- **Interface Status:** ‚úÖ Working
- **Port:** 7860

## üìã Usage Instructions
1. **Q&A Mode:** Ask questions to multiple models
2. **Debate Mode:** Set up debates between models
3. **Status:** View system information

**Note:** This is a working Gradio interface for the Multi-Model AI Assistant!
"""
                    
                    refresh_status.click(refresh_status_data, outputs=status_output)
            
            # Footer
            gr.HTML(f"""
            <div style="text-align: center; margin-top: 2rem; padding: 1rem; background: #f8f9fa; border-radius: 8px;">
                <p><strong>üéâ Gradio App is Working!</strong></p>
                <p><strong>Session ID:</strong> {self.session_id[:8]}... | 
                <strong>Models:</strong> {len(self.models)} | 
                <strong>Framework:</strong> Gradio {gr.__version__}</p>
            </div>
            """)
        
        return interface
    
    def launch(self, **kwargs):
        """Launch the Gradio interface"""
        print("üöÄ Launching Gradio interface...")
        interface = self.create_interface()
        return interface.launch(**kwargs)

# Create and launch the app
if __name__ == "__main__":
    print("üöÄ Starting Simple Gradio App...")
    app = SimpleGradioApp()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )
